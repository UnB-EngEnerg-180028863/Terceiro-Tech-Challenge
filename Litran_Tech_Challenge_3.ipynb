{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e05d6991",
   "metadata": {},
   "source": [
    "## Tech Challenge - Fine Tuning de Foundation Model\n",
    "**Aluno:** Vin√≠cius Oliveira Litran Andrade\n",
    "\n",
    "Neste projeto, realizo o fine-tuning de um modelo de linguagem utilizando o dataset **AmazonTitles-1.3MM**, que cont√©m t√≠tulos de produtos e suas respectivas descri√ß√µes.\n",
    "\n",
    "O objetivo √© treinar o modelo para, a partir de uma pergunta baseada no t√≠tulo do produto, gerar uma resposta coerente com base na descri√ß√£o aprendida durante o treinamento.\n",
    "\n",
    "Devido √†s limita√ß√µes de hardware, o modelo utilizado foi o **DistilGPT-2**, uma vers√£o otimizada e menor do GPT-2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4243d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# üì¶ Importa√ß√£o de bibliotecas necess√°rias\n",
    "# =========================================\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import unicodedata\n",
    "import html\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForCausalLM,\n",
    "    Trainer, TrainingArguments,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from datasets import load_dataset\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56dbf25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# üîß Inicializa√ß√£o do Tokenizer e do Modelo Base\n",
    "# =========================================\n",
    "\n",
    "model_name = 'distilgpt2'\n",
    "\n",
    "# Carrega o tokenizer e o modelo base\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16789229",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    }
   ],
   "source": [
    "# Adiciona token de padding se n√£o existir\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b6c73d",
   "metadata": {},
   "source": [
    "## Prepara√ß√£o dos Dados\n",
    "\n",
    "O dataset original cont√©m dois campos principais: \n",
    "- **Title:** T√≠tulo do produto.\n",
    "- **Content:** Descri√ß√£o detalhada do produto.\n",
    "\n",
    "O processo de prepara√ß√£o incluiu:\n",
    "- Limpeza dos textos para remover caracteres n√£o imprim√≠veis.\n",
    "- Cria√ß√£o de prompts no formato: \n",
    "\n",
    "You are an assistant that answers questions based on the product description.\n",
    "\n",
    "Question: {title}\n",
    "Answer: {content}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f306c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# üßπ Fun√ß√µes de Pr√©-processamento dos Dados\n",
    "# =========================================\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Limpa e normaliza texto, removendo HTML, caracteres especiais e espa√ßos desnecess√°rios.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return \"\"\n",
    "    text = unicodedata.normalize(\"NFKC\", text)                 # Normaliza acentua√ß√£o\n",
    "    text = re.sub(r'<[^>]+>', ' ', text)                      # Remove tags HTML\n",
    "    text = html.unescape(text)                                # Converte entidades HTML (&amp;)\n",
    "    text = re.sub(r'[^\\x20-\\x7E]+', ' ', text)                # Remove caracteres n√£o ASCII\n",
    "    text = re.sub(r'\\s+', ' ', text)                          # Remove m√∫ltiplos espa√ßos\n",
    "    text = re.sub(r'[\\r\\n\\t]', ' ', text)                     # Remove quebras de linha e tabula√ß√µes\n",
    "    text = re.sub(r'\\s([?.!,:;\"])', r'\\1', text)              # Remove espa√ßo antes de pontua√ß√£o\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e090776",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens(text):\n",
    "    \"\"\"\n",
    "    Conta quantos tokens o texto possui de acordo com o tokenizer.\n",
    "    \"\"\"\n",
    "    return len(tokenizer.encode(text, add_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1b0ac59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(input_path, output_path, min_tokens=10, max_total_tokens=256, percent_dataset=0.1):\n",
    "    \"\"\"\n",
    "    Processa o dataset JSONL original e gera um dataset formatado com perguntas e respostas.\n",
    "    \"\"\"\n",
    "    output_file = Path(output_path)\n",
    "    num_written = 0\n",
    "\n",
    "    with open(input_path, 'r', encoding='utf-8') as f:\n",
    "        total_lines = sum(1 for _ in f)  # Conta total de linhas (exemplos)\n",
    "\n",
    "    max_examples = int(total_lines * percent_dataset)  # Define amostragem\n",
    "\n",
    "    with open(input_path, 'r', encoding='utf-8') as infile, \\\n",
    "         open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "\n",
    "        for line in tqdm(infile, desc=\"Processando\"):\n",
    "            if num_written >= max_examples:\n",
    "                break\n",
    "\n",
    "            item = json.loads(line)\n",
    "            title = clean_text(item.get('title', ''))\n",
    "            content = clean_text(item.get('content', ''))\n",
    "\n",
    "            if not title or not content:\n",
    "                continue\n",
    "\n",
    "            # Formata prompt de maneira mais clara para o modelo\n",
    "            text = f\"\"\"\n",
    "            You are an assistant that answers questions based on the product description.\n",
    "\n",
    "            Question: {title}\n",
    "            Answer: {content}\n",
    "            \"\"\".strip()\n",
    "\n",
    "            num_tokens = count_tokens(text)\n",
    "\n",
    "            if num_tokens < min_tokens:\n",
    "                continue\n",
    "            if num_tokens > max_total_tokens:\n",
    "                continue\n",
    "\n",
    "            example = {\"text\": text}\n",
    "            outfile.write(json.dumps(example, ensure_ascii=False) + '\\n')\n",
    "            num_written += 1\n",
    "\n",
    "    print(f\"\\n‚úÖ Dados salvos em {output_file} com {num_written} exemplos.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9de70d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando: 525it [00:00, 5236.46it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1127 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Processando: 558809it [02:39, 3506.46it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Dados salvos em fine_tune_data.jsonl com 224861 exemplos.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "003d3f9b9fba4689a32b6aad7425ed96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =========================================\n",
    "# üöÄ Prepara√ß√£o e Tokeniza√ß√£o do Dataset\n",
    "# =========================================\n",
    "\n",
    "# Executa a prepara√ß√£o dos dados\n",
    "prepare_data(\n",
    "    input_path='LF-Amazon-1.3M/trn.json',\n",
    "    output_path='fine_tune_data.jsonl',\n",
    "    min_tokens=5,\n",
    "    max_total_tokens=256\n",
    ")\n",
    "\n",
    "# Carrega dataset no formato JSONL\n",
    "dataset = load_dataset('json', data_files={'train': 'fine_tune_data.jsonl'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4507fb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "459e0be89f9246629ba089210df44b7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/224861 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =========================================\n",
    "# ‚úçÔ∏è Tokeniza√ß√£o\n",
    "# =========================================\n",
    "\n",
    "# Fun√ß√£o para tokeniza√ß√£o dos dados\n",
    "def tokenize_function(examples):\n",
    "    tokenized = tokenizer(\n",
    "        examples['text'],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=256\n",
    "    )\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()  # Para Language Modeling, entrada = sa√≠da\n",
    "    return tokenized\n",
    "\n",
    "# Aplica tokeniza√ß√£o\n",
    "tokenized_datasets = dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=['text'],\n",
    "    num_proc=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaffd792",
   "metadata": {},
   "source": [
    "## üéØ Fine-Tuning do Modelo\n",
    "\n",
    "O fine-tuning foi realizado utilizando o modelo **DistilGPT-2**, uma vers√£o otimizada e compacta do GPT-2, que possui aproximadamente **82 milh√µes de par√¢metros**, em compara√ß√£o com os **124 milh√µes do GPT-2 original**.\n",
    "\n",
    "---\n",
    "\n",
    "### üîç Justificativa da Escolha do Modelo\n",
    "\n",
    "O modelo **DistilGPT-2** foi selecionado por ser:\n",
    "\n",
    "- ‚ö° **Leve e r√°pido de treinar.**  \n",
    "- üíª **Menos demandante em termos de recursos computacionais.**  \n",
    "- üè† **Vi√°vel para execu√ß√£o local em m√°quinas com GPUs de consumo**, como **RTX 4070**, **RTX 3060** ou **RTX 2060**.  \n",
    "- üîÑ **Capaz de executar todo o ciclo de:**\n",
    "  - **Pr√©-processamento**\n",
    "  - **Treinamento (fine-tuning)**\n",
    "  - **Gera√ß√£o de respostas**\n",
    "\n",
    "‚û°Ô∏è Tudo isso **sem custos adicionais com infraestrutura em nuvem**, dentro de prazos compat√≠veis com as demandas acad√™micas.\n",
    "\n",
    "> ‚ö†Ô∏è Embora o desempenho n√£o seja compar√°vel a modelos maiores, como **Llama 2**, **Mistral** ou **GPT-3**, ele √© plenamente suficiente para **validar o pipeline completo de fine-tuning aplicado ao dataset _AmazonTitles-1.3MM_**.\n",
    "\n",
    "---\n",
    "\n",
    "## üîß Par√¢metros do Fine-Tuning\n",
    "\n",
    "| **Par√¢metro**         | **Valor**            | **Justificativa**                                                            |\n",
    "|-----------------------|----------------------|------------------------------------------------------------------------------|\n",
    "| **Modelo base**       | `distilgpt2`         | Leve, otimizado, adequado para rodar localmente                              |\n",
    "| **Batch Size**        | `16`                 | Equil√≠brio entre consumo de mem√≥ria e velocidade de treinamento              |\n",
    "| **√âpocas (Epochs)**   | `3`                  | Reduz tempo de treino e risco de overfitting                                 |\n",
    "| **Max Tokens**        | `256`                | Limita o contexto ao tamanho que o modelo suporta                            |\n",
    "| **Learning Rate**     | `3e-4`               | Valor padr√£o eficaz para modelos autoregressivos                             |\n",
    "| **Weight Decay**      | `0.01`               | Regulariza√ß√£o que ajuda a prevenir overfitting                               |\n",
    "| **FP16 Precision**    | `True` (se CUDA)     | Acelera o treino e reduz consumo de mem√≥ria se houver GPU                    |\n",
    "| **Save Steps**        | `500`                | Salva checkpoints a cada 500 passos                                          |\n",
    "| **Save Total Limit**  | `1`                  | Mant√©m apenas o checkpoint mais recente para economizar espa√ßo               |\n",
    "| **Logging Steps**     | `100`                | Frequ√™ncia dos logs durante o treinamento                                    |\n",
    "\n",
    "---\n",
    "\n",
    "## üö© Observa√ß√µes Importantes\n",
    "\n",
    "- O uso de **apenas 3 √©pocas** foi suficiente, considerando:\n",
    "  - üìä O tamanho do modelo.\n",
    "  - üîß A simplicidade da tarefa (gerar respostas a partir de descri√ß√µes).\n",
    "  - ‚ö†Ô∏è O risco de **overfitting**, que cresce rapidamente em modelos menores com mais √©pocas.\n",
    "\n",
    "- Para tarefas mais complexas ou modelos maiores, seriam necess√°rias:\n",
    "  - üîº Mais √©pocas de treinamento.\n",
    "  - ‚òÅÔ∏è Recursos em cloud computing.\n",
    "  - üéØ Ajustes mais refinados nos hiperpar√¢metros.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af8dd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# ‚öôÔ∏è Configura√ß√£o do Fine-Tuning\n",
    "# =========================================\n",
    "\n",
    "output_dir = \"fine_tuned_model\"\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False  # False porque GPT-2 √© um modelo causal, n√£o masked\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    save_steps=500,\n",
    "    save_total_limit=1,\n",
    "    logging_steps=100,\n",
    "    learning_rate=5e-5,  # Valor mais est√°vel para esse tipo de tarefa\n",
    "    weight_decay=0.01,\n",
    "    fp16=torch.cuda.is_available(),  # Usa meia precis√£o se houver GPU\n",
    "    push_to_hub=False,\n",
    "    report_to=\"none\",\n",
    "    optim=\"adamw_torch_fused\",  # Otimizador mais moderno e eficiente\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b15849",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50258, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-5): 6 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50258, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =========================================\n",
    "# üèãÔ∏è Execu√ß√£o do Fine-Tuning\n",
    "# =========================================\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "# Move modelo para GPU, se dispon√≠vel\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83a0a9f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='42162' max='42162' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [42162/42162 1:28:45, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.062100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.881900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.859400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.825800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.821000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>2.782300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>2.784700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>2.774700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>2.754000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.716700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>2.733900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>2.727400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>2.712200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>2.739900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.740800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>2.720700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>2.713400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>2.703700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>2.697800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.717300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>2.680000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>2.668700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>2.679800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>2.686700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>2.684900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>2.682400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>2.687700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>2.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>2.668800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>2.638100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>2.640200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>2.650400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>2.703600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>2.618200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>2.661900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>2.652500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>2.654600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>2.648100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>2.640700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>2.654100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>2.652500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>2.634000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>2.651600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>2.626000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>2.614300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>2.646700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>2.643900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>2.599700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>2.606900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>2.612300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>2.622400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>2.615300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>2.620800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>2.604800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>2.636900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>2.591100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5700</td>\n",
       "      <td>2.596300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>2.589300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5900</td>\n",
       "      <td>2.602500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>2.601200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6100</td>\n",
       "      <td>2.581300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>2.595500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6300</td>\n",
       "      <td>2.591500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>2.597300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>2.610700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>2.599000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6700</td>\n",
       "      <td>2.598100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>2.581000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6900</td>\n",
       "      <td>2.601900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>2.561100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7100</td>\n",
       "      <td>2.608500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>2.598900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7300</td>\n",
       "      <td>2.582100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>2.591100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>2.576800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>2.562000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7700</td>\n",
       "      <td>2.560400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>2.577300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7900</td>\n",
       "      <td>2.591100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>2.554100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8100</td>\n",
       "      <td>2.599500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8200</td>\n",
       "      <td>2.570300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8300</td>\n",
       "      <td>2.556900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8400</td>\n",
       "      <td>2.547500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>2.550800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8600</td>\n",
       "      <td>2.578500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8700</td>\n",
       "      <td>2.533600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8800</td>\n",
       "      <td>2.585600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8900</td>\n",
       "      <td>2.563000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>2.569600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9100</td>\n",
       "      <td>2.550500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9200</td>\n",
       "      <td>2.550800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9300</td>\n",
       "      <td>2.577900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9400</td>\n",
       "      <td>2.569700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>2.555000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9600</td>\n",
       "      <td>2.569900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9700</td>\n",
       "      <td>2.544100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9800</td>\n",
       "      <td>2.560900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9900</td>\n",
       "      <td>2.524500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>2.567900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10100</td>\n",
       "      <td>2.558600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10200</td>\n",
       "      <td>2.575000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10300</td>\n",
       "      <td>2.531800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10400</td>\n",
       "      <td>2.533000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>2.544700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10600</td>\n",
       "      <td>2.540200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10700</td>\n",
       "      <td>2.532100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10800</td>\n",
       "      <td>2.555100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10900</td>\n",
       "      <td>2.548300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>2.537700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11100</td>\n",
       "      <td>2.508500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11200</td>\n",
       "      <td>2.548400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11300</td>\n",
       "      <td>2.548700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11400</td>\n",
       "      <td>2.550000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>2.546400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11600</td>\n",
       "      <td>2.535200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11700</td>\n",
       "      <td>2.562100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11800</td>\n",
       "      <td>2.524900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11900</td>\n",
       "      <td>2.563400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>2.545800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12100</td>\n",
       "      <td>2.508300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12200</td>\n",
       "      <td>2.513100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12300</td>\n",
       "      <td>2.519800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12400</td>\n",
       "      <td>2.518600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>2.561300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12600</td>\n",
       "      <td>2.533100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12700</td>\n",
       "      <td>2.528900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12800</td>\n",
       "      <td>2.565100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12900</td>\n",
       "      <td>2.520100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>2.512300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13100</td>\n",
       "      <td>2.530000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13200</td>\n",
       "      <td>2.535400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13300</td>\n",
       "      <td>2.562800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13400</td>\n",
       "      <td>2.521900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>2.558600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13600</td>\n",
       "      <td>2.544300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13700</td>\n",
       "      <td>2.551900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13800</td>\n",
       "      <td>2.518600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13900</td>\n",
       "      <td>2.520200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>2.535400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14100</td>\n",
       "      <td>2.492800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14200</td>\n",
       "      <td>2.472700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14300</td>\n",
       "      <td>2.475300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14400</td>\n",
       "      <td>2.487200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>2.471800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14600</td>\n",
       "      <td>2.474500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14700</td>\n",
       "      <td>2.461200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14800</td>\n",
       "      <td>2.480700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14900</td>\n",
       "      <td>2.476000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>2.497800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15100</td>\n",
       "      <td>2.491200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15200</td>\n",
       "      <td>2.490500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15300</td>\n",
       "      <td>2.476000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15400</td>\n",
       "      <td>2.488600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>2.482400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15600</td>\n",
       "      <td>2.464200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15700</td>\n",
       "      <td>2.464900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15800</td>\n",
       "      <td>2.463000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15900</td>\n",
       "      <td>2.485700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>2.473200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16100</td>\n",
       "      <td>2.479600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16200</td>\n",
       "      <td>2.485800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16300</td>\n",
       "      <td>2.478500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16400</td>\n",
       "      <td>2.458100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>2.446300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16600</td>\n",
       "      <td>2.469100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16700</td>\n",
       "      <td>2.445800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16800</td>\n",
       "      <td>2.469700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16900</td>\n",
       "      <td>2.505900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>2.448700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17100</td>\n",
       "      <td>2.495400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17200</td>\n",
       "      <td>2.455600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17300</td>\n",
       "      <td>2.477900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17400</td>\n",
       "      <td>2.456500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>2.468700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17600</td>\n",
       "      <td>2.492000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17700</td>\n",
       "      <td>2.454200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17800</td>\n",
       "      <td>2.475600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17900</td>\n",
       "      <td>2.462500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>2.485800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18100</td>\n",
       "      <td>2.444600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18200</td>\n",
       "      <td>2.439900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18300</td>\n",
       "      <td>2.462100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18400</td>\n",
       "      <td>2.471800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>2.464000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18600</td>\n",
       "      <td>2.449000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18700</td>\n",
       "      <td>2.447900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18800</td>\n",
       "      <td>2.463800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18900</td>\n",
       "      <td>2.469500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>2.496800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19100</td>\n",
       "      <td>2.453400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19200</td>\n",
       "      <td>2.486400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19300</td>\n",
       "      <td>2.452600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19400</td>\n",
       "      <td>2.447000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>2.449600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19600</td>\n",
       "      <td>2.465700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19700</td>\n",
       "      <td>2.455500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19800</td>\n",
       "      <td>2.441500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19900</td>\n",
       "      <td>2.453500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>2.458000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20100</td>\n",
       "      <td>2.461300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20200</td>\n",
       "      <td>2.419900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20300</td>\n",
       "      <td>2.468000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20400</td>\n",
       "      <td>2.463900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20500</td>\n",
       "      <td>2.469300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20600</td>\n",
       "      <td>2.436100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20700</td>\n",
       "      <td>2.484600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20800</td>\n",
       "      <td>2.450000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20900</td>\n",
       "      <td>2.440300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>2.434900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21100</td>\n",
       "      <td>2.475800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21200</td>\n",
       "      <td>2.459100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21300</td>\n",
       "      <td>2.443000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21400</td>\n",
       "      <td>2.468700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21500</td>\n",
       "      <td>2.469500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21600</td>\n",
       "      <td>2.490100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21700</td>\n",
       "      <td>2.449400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21800</td>\n",
       "      <td>2.450400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21900</td>\n",
       "      <td>2.449400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>2.449400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22100</td>\n",
       "      <td>2.433000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22200</td>\n",
       "      <td>2.440800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22300</td>\n",
       "      <td>2.441900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22400</td>\n",
       "      <td>2.447700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22500</td>\n",
       "      <td>2.469600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22600</td>\n",
       "      <td>2.459400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22700</td>\n",
       "      <td>2.477700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22800</td>\n",
       "      <td>2.493200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22900</td>\n",
       "      <td>2.471100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>2.436900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23100</td>\n",
       "      <td>2.456100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23200</td>\n",
       "      <td>2.446300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23300</td>\n",
       "      <td>2.462100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23400</td>\n",
       "      <td>2.449500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23500</td>\n",
       "      <td>2.435300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23600</td>\n",
       "      <td>2.444500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23700</td>\n",
       "      <td>2.447700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23800</td>\n",
       "      <td>2.406600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23900</td>\n",
       "      <td>2.452400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>2.453500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24100</td>\n",
       "      <td>2.454900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24200</td>\n",
       "      <td>2.464300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24300</td>\n",
       "      <td>2.431200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24400</td>\n",
       "      <td>2.484200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24500</td>\n",
       "      <td>2.440700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24600</td>\n",
       "      <td>2.481300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24700</td>\n",
       "      <td>2.428100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24800</td>\n",
       "      <td>2.454400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24900</td>\n",
       "      <td>2.456100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>2.449100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25100</td>\n",
       "      <td>2.409500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25200</td>\n",
       "      <td>2.451300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25300</td>\n",
       "      <td>2.446900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25400</td>\n",
       "      <td>2.422000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25500</td>\n",
       "      <td>2.420400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25600</td>\n",
       "      <td>2.418500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25700</td>\n",
       "      <td>2.406500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25800</td>\n",
       "      <td>2.436400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25900</td>\n",
       "      <td>2.433100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>2.420300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26100</td>\n",
       "      <td>2.409700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26200</td>\n",
       "      <td>2.454800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26300</td>\n",
       "      <td>2.434600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26400</td>\n",
       "      <td>2.450200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26500</td>\n",
       "      <td>2.443400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26600</td>\n",
       "      <td>2.443700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26700</td>\n",
       "      <td>2.448500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26800</td>\n",
       "      <td>2.419300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26900</td>\n",
       "      <td>2.456400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>2.428600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27100</td>\n",
       "      <td>2.435000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27200</td>\n",
       "      <td>2.476800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27300</td>\n",
       "      <td>2.418600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27400</td>\n",
       "      <td>2.430400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27500</td>\n",
       "      <td>2.466300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27600</td>\n",
       "      <td>2.440700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27700</td>\n",
       "      <td>2.420100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27800</td>\n",
       "      <td>2.447500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27900</td>\n",
       "      <td>2.433700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>2.417400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28100</td>\n",
       "      <td>2.409500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28200</td>\n",
       "      <td>2.424800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28300</td>\n",
       "      <td>2.395500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28400</td>\n",
       "      <td>2.399600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28500</td>\n",
       "      <td>2.394400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28600</td>\n",
       "      <td>2.400300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28700</td>\n",
       "      <td>2.406300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28800</td>\n",
       "      <td>2.408400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28900</td>\n",
       "      <td>2.414700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>2.418200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29100</td>\n",
       "      <td>2.422700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29200</td>\n",
       "      <td>2.393200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29300</td>\n",
       "      <td>2.430300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29400</td>\n",
       "      <td>2.407400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29500</td>\n",
       "      <td>2.396900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29600</td>\n",
       "      <td>2.386500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29700</td>\n",
       "      <td>2.383200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29800</td>\n",
       "      <td>2.404900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29900</td>\n",
       "      <td>2.406800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>2.394100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30100</td>\n",
       "      <td>2.390100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30200</td>\n",
       "      <td>2.386500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30300</td>\n",
       "      <td>2.376800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30400</td>\n",
       "      <td>2.393300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30500</td>\n",
       "      <td>2.404000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30600</td>\n",
       "      <td>2.418200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30700</td>\n",
       "      <td>2.381300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30800</td>\n",
       "      <td>2.427700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30900</td>\n",
       "      <td>2.396600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>2.389300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31100</td>\n",
       "      <td>2.424800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31200</td>\n",
       "      <td>2.384300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31300</td>\n",
       "      <td>2.427000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31400</td>\n",
       "      <td>2.391300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31500</td>\n",
       "      <td>2.419300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31600</td>\n",
       "      <td>2.421000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31700</td>\n",
       "      <td>2.414700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31800</td>\n",
       "      <td>2.419200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31900</td>\n",
       "      <td>2.404300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>2.413400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32100</td>\n",
       "      <td>2.399900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32200</td>\n",
       "      <td>2.402200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32300</td>\n",
       "      <td>2.405700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32400</td>\n",
       "      <td>2.398700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32500</td>\n",
       "      <td>2.386700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32600</td>\n",
       "      <td>2.398100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32700</td>\n",
       "      <td>2.406800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32800</td>\n",
       "      <td>2.437800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32900</td>\n",
       "      <td>2.405900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>2.387000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33100</td>\n",
       "      <td>2.391900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33200</td>\n",
       "      <td>2.419900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33300</td>\n",
       "      <td>2.366700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33400</td>\n",
       "      <td>2.409300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33500</td>\n",
       "      <td>2.417700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33600</td>\n",
       "      <td>2.372300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33700</td>\n",
       "      <td>2.402400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33800</td>\n",
       "      <td>2.418000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33900</td>\n",
       "      <td>2.400300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>2.388300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34100</td>\n",
       "      <td>2.409100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34200</td>\n",
       "      <td>2.420100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34300</td>\n",
       "      <td>2.395100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34400</td>\n",
       "      <td>2.389100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34500</td>\n",
       "      <td>2.402400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34600</td>\n",
       "      <td>2.415800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34700</td>\n",
       "      <td>2.421600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34800</td>\n",
       "      <td>2.374500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34900</td>\n",
       "      <td>2.389700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>2.399900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35100</td>\n",
       "      <td>2.403300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35200</td>\n",
       "      <td>2.386900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35300</td>\n",
       "      <td>2.396000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35400</td>\n",
       "      <td>2.414200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35500</td>\n",
       "      <td>2.399600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35600</td>\n",
       "      <td>2.401700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35700</td>\n",
       "      <td>2.419400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35800</td>\n",
       "      <td>2.412100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35900</td>\n",
       "      <td>2.381600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>2.378600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36100</td>\n",
       "      <td>2.417600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36200</td>\n",
       "      <td>2.386400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36300</td>\n",
       "      <td>2.381900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36400</td>\n",
       "      <td>2.389900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36500</td>\n",
       "      <td>2.397400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36600</td>\n",
       "      <td>2.407100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36700</td>\n",
       "      <td>2.387200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36800</td>\n",
       "      <td>2.369700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36900</td>\n",
       "      <td>2.392700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37000</td>\n",
       "      <td>2.382100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37100</td>\n",
       "      <td>2.391000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37200</td>\n",
       "      <td>2.433900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37300</td>\n",
       "      <td>2.410900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37400</td>\n",
       "      <td>2.409800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37500</td>\n",
       "      <td>2.352300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37600</td>\n",
       "      <td>2.379500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37700</td>\n",
       "      <td>2.409900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37800</td>\n",
       "      <td>2.377500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37900</td>\n",
       "      <td>2.401200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38000</td>\n",
       "      <td>2.372200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38100</td>\n",
       "      <td>2.418100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38200</td>\n",
       "      <td>2.398900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38300</td>\n",
       "      <td>2.390300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38400</td>\n",
       "      <td>2.364800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38500</td>\n",
       "      <td>2.412200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38600</td>\n",
       "      <td>2.381900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38700</td>\n",
       "      <td>2.360400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38800</td>\n",
       "      <td>2.385600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38900</td>\n",
       "      <td>2.392700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39000</td>\n",
       "      <td>2.379600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39100</td>\n",
       "      <td>2.416200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39200</td>\n",
       "      <td>2.375800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39300</td>\n",
       "      <td>2.394700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39400</td>\n",
       "      <td>2.381200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39500</td>\n",
       "      <td>2.389400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39600</td>\n",
       "      <td>2.384700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39700</td>\n",
       "      <td>2.390100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39800</td>\n",
       "      <td>2.396700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39900</td>\n",
       "      <td>2.367300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>2.401600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40100</td>\n",
       "      <td>2.378400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40200</td>\n",
       "      <td>2.379900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40300</td>\n",
       "      <td>2.402500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40400</td>\n",
       "      <td>2.389900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40500</td>\n",
       "      <td>2.416400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40600</td>\n",
       "      <td>2.376800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40700</td>\n",
       "      <td>2.382000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40800</td>\n",
       "      <td>2.373100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40900</td>\n",
       "      <td>2.388300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41000</td>\n",
       "      <td>2.401600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41100</td>\n",
       "      <td>2.368600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41200</td>\n",
       "      <td>2.396400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41300</td>\n",
       "      <td>2.406600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41400</td>\n",
       "      <td>2.402100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41500</td>\n",
       "      <td>2.418300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41600</td>\n",
       "      <td>2.382400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41700</td>\n",
       "      <td>2.390800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41800</td>\n",
       "      <td>2.391700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41900</td>\n",
       "      <td>2.397200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42000</td>\n",
       "      <td>2.394700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42100</td>\n",
       "      <td>2.399400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Modelo fine-tuned salvo em fine_tuned_model\n"
     ]
    }
   ],
   "source": [
    "# Inicia treinamento\n",
    "trainer.train()\n",
    "\n",
    "# Salva modelo e tokenizer treinados\n",
    "trainer.save_model(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(f\"‚úÖ Modelo fine-tuned salvo em {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "51bbd647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# üîó Fun√ß√£o para Carregar o Modelo Treinado\n",
    "# =========================================\n",
    "\n",
    "def load_model(model_path):\n",
    "    \"\"\"\n",
    "    Carrega o modelo e tokenizer treinados a partir do diret√≥rio.\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "    model.eval()\n",
    "    return tokenizer, model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f481e8",
   "metadata": {},
   "source": [
    "## Gera√ß√£o de Respostas\n",
    "\n",
    "Ap√≥s o treinamento, o modelo foi configurado para receber perguntas no seguinte formato:\n",
    "\n",
    "```plaintext\n",
    "Pergunta: {t√≠tulo do produto}\n",
    "Resposta:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fe5e1702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# üó£Ô∏è Fun√ß√£o de Gera√ß√£o de Respostas\n",
    "# =========================================\n",
    "\n",
    "def gerar_resposta(pergunta, tokenizer, model, max_length=150):\n",
    "    \"\"\"\n",
    "    Gera uma resposta a partir de uma pergunta fornecida.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    You are an assistant that answers questions based on the product description.\n",
    "\n",
    "    Question: {pergunta}\n",
    "    Answer:\n",
    "    \"\"\".strip()\n",
    "\n",
    "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    outputs = model.generate(\n",
    "        inputs,\n",
    "        max_length=max_length,\n",
    "        num_return_sequences=1,\n",
    "        do_sample=True,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        temperature=0.7,\n",
    "        pad_token_id=tokenizer.pad_token_id\n",
    "    )\n",
    "\n",
    "    resposta = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    resposta = resposta.split(\"Resposta:\")[-1].strip()\n",
    "\n",
    "    return resposta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "321b01db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîç Fun√ß√£o de teste do modelo (antes e depois do fine-tuning)\n",
    "\n",
    "def testar_modelo(pergunta, tokenizer, model, max_length=150):\n",
    "    \"\"\"\n",
    "    Gera uma resposta para a pergunta fornecida.\n",
    "    Funciona tanto antes quanto depois do fine-tuning.\n",
    "    \"\"\"\n",
    "    prompt = f\"Pergunta: {pergunta}\\nResposta:\"\n",
    "    \n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors='pt',\n",
    "        padding=True\n",
    "    ).to(model.device)\n",
    "\n",
    "    outputs = model.generate(\n",
    "        inputs.input_ids,\n",
    "        attention_mask=inputs.attention_mask,\n",
    "        max_length=max_length,\n",
    "        num_return_sequences=1,\n",
    "        no_repeat_ngram_size=2,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    resposta = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Extrai apenas o conte√∫do ap√≥s \"Resposta:\"\n",
    "    if \"Resposta:\" in resposta:\n",
    "        resposta = resposta.split(\"Resposta:\")[-1]\n",
    "    if \"Pergunta:\" in resposta:\n",
    "        resposta = resposta.split(\"Pergunta:\")[0]\n",
    "\n",
    "    return resposta.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4275654c",
   "metadata": {},
   "outputs": [],
   "source": [
    "perguntas = [\n",
    "    # Pergunta baseada em \"Girls Ballet Tutu Neon Pink\" ou \"Girls Ballet Tutu Neon Blue\"\n",
    "    \"Tell me about the Girls Ballet Tutu Neon Pink.\", \n",
    "    # Pergunta baseada em \"Mog's Kittens\"\n",
    "    \"What kind of book is Mog's Kittens?\",\n",
    "    # Pergunta baseada em \"The Prophet\"\n",
    "    \"Can you describe The Prophet by Gibran?\",\n",
    "    # Pergunta baseada em \"Spirit Led-Moving By Grace In The Holy Spirit's Gifts\"\n",
    "    \"What is the content of Spirit Led-Moving By Grace In The Holy Spirit's Gifts?\",\n",
    "    # Pergunta baseada em \"Mexico The Beautiful Cookbook\" ou \"America: The Beautiful Cookbook\"\n",
    "    \"What is special about the Mexico The Beautiful Cookbook?\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7ae3698f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† üîç Respostas ANTES do fine-tuning:\n",
      "\n",
      "üî∏ Pergunta: Tell me about the Girls Ballet Tutu Neon Pink.\n",
      "üîπ Resposta antes do fine-tuning: The Only Time I Ever Had to Get a Job\n",
      "This book has been a labor of love for girls since I was a child. I have been the best job I've ever had to get a job since reading this book. If you are interested in girls and girls, this is the book for you. --The Author, Author of the Best Girls' Books Ever! I am excited to see how you have helped me with this wonderful book! --Marianne St. John, Founder and President, Inc.I wish I had this one in my books. It is an inspiring book that I would have loved to read. Thank you\n",
      "\n",
      "üî∏ Pergunta: What kind of book is Mog's Kittens?\n",
      "üîπ Resposta antes do fine-tuning: The Complete Guide to the Mog Mog\n",
      "Mog: A Complete Book of Mog (Volume 1)\n",
      "The Mog: Complete Series: Mog, a Complete Volume 1 (also called the Complete Mog Series) (Mogg, the Ultimate Guide) and Mog Guide (The Ultimate Guides to Mog). The book includes over 50 maps and maps, and a glossary of terms and terms. You'll also find the most up-to-date Mog encyclopedia of all time. The Mog is a collection of the best Mog maps available, with a wealth of information and information on the world of modern Mog. This series is designed to be used by anyone interested in\n",
      "\n",
      "üî∏ Pergunta: Can you describe The Prophet by Gibran?\n",
      "üîπ Resposta antes do fine-tuning: The Complete Idiot's Guide to the Holy Qur'an\n",
      "In the context of the Quranic and Islamic Tradition, you will find:\n",
      "* An exhaustive introduction to Islamic and Muslim sources* A comprehensive bibliography of Quran texts* The complete bibliographic index of each book* All the major sources of information from the Koran* Introduction to Arabic and Persian Islamic sources * Includes a bimonthly b biblically annotated bibles* Exhibits an extensive bibliofication index* For example, the bible of a Bible is\"a treasure of all things.\"* This is the most complete work on the subject available\n",
      "\n",
      "üî∏ Pergunta: What is the content of Spirit Led-Moving By Grace In The Holy Spirit's Gifts?\n",
      "üîπ Resposta antes do fine-tuning: A Journey Through The Heart of the Holy Trinity\n",
      "The Spirit in the Spirit of Grace in The Sacred Heart (Volume 2)\n",
      "Spirit of Freedom: The Spirit and the Sacred Spirit (Spirituality in Spirituality) (The Spiritual Series) is a compendium of spiritual and practical teachings and teachings from the best-selling author, Dr. G.A. Jung. The volume is divided into four parts: the\"spiritual\" and\"transpersonal\" parts of a spiritual journey. Each chapter contains a brief summary of each chapter. Written in a clear, easy-to-read style, the book has\n",
      "\n",
      "üî∏ Pergunta: What is special about the Mexico The Beautiful Cookbook?\n",
      "üîπ Resposta antes do fine-tuning: A Taste of Mexican Cuisine\n",
      "\n",
      "The recipes in this book are the perfect companion to the book on the subject. Recipes are made with fresh ingredients and a good sense of the food. This is a great book for anyone interested in Mexican cuisine. --Margaret M. Boon, author of The Mexican Cooking Book and The New Mexican Cook Book: From the Inside OutThis book is an essential reference for any serious cook. I found it very useful and enjoyable. It would be an excellent addition to any library that has a culinary library. (Gina W. Wahl, San Diego State University)This is the first book in a series\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"üß† üîç Respostas ANTES do fine-tuning:\\n\")\n",
    "\n",
    "for pergunta in perguntas:\n",
    "    resposta = testar_modelo(pergunta, tokenizer, model)\n",
    "    print(f\"üî∏ Pergunta: {pergunta}\")\n",
    "    print(f\"üîπ Resposta antes do fine-tuning: {resposta}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c1a34eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† üöÄ Respostas DEPOIS do fine-tuning:\n",
      "\n",
      "üî∏ Pergunta: Tell me about the Girls Ballet Tutu Neon Pink.\n",
      "üîπ Resposta depois do fine-tuning: The Story of a Novel\n",
      "\n",
      "The story of the storyteller at work in the 1950s and 60s\n",
      "A new book by a young girl at the age of nine tells of her first day as a girl and her love for art and music. --Rendezvous, September, 2001From the Foreword by Lesley P. Smith, aNew York Timesbestselling author of The Lady Who Wrote The Book of My LifeA gripping story about a talented young woman who was the first woman to earn a living and was a successful artist. In her debut novel, The Girl Who Readed the Book Of My Dreams, she\n",
      "\n",
      "üî∏ Pergunta: What kind of book is Mog's Kittens?\n",
      "üîπ Resposta depois do fine-tuning: A Very Short Introduction\n",
      "SOG's Masks in the World of Black Catacombs\n",
      "The book's first section is a primer for the Moges and other related subjects in this series, with a glossary of terms and a detailed index. The second section covers the history and origin of the Kitten, a very specialized catacomb and its related creatures. Also included are the various breeds of kittens, the origin, and their origins, as well as the origins and evolution of each species.The first two chapters cover the evolution and origins of these kitten. Each chapter concludes with an overview of various aspects of Kinesiology, including\n",
      "\n",
      "üî∏ Pergunta: Can you describe The Prophet by Gibran?\n",
      "üîπ Resposta depois do fine-tuning: The History of a Catholic Church\n",
      "The text is a rich and rich source of information about the history of the Catholic church. This book is the most comprehensive introduction to this subject available. --Catholic Historical Quarterly, Vol. 16, No. 2, March 2000It is an impressive and well-organized book, with a wealth of material that is well laid out and written. It is written in a very clear, concise style and contains an extensive bibliography, a glossary and a bibliographical index. I am very impressed with the quality of coverage and the use of illustrations. The book will be of great interest to those who are interested in the subject\n",
      "\n",
      "üî∏ Pergunta: What is the content of Spirit Led-Moving By Grace In The Holy Spirit's Gifts?\n",
      "üîπ Resposta depois do fine-tuning: In the Spirit of Grace in the Holy Family\n",
      "The Spirit in Spirit\n",
      "I am a believer who has a profound understanding of the power of God and his gifts, and I believe that this book will help you find the right way to live a life of love and devotion. I will say that it is a powerful gift to be a part of Christ's life. --Mary B. Davis, M.D., author ofThe Power of Spiritual Spirituality: The Power to Heal Your Life,andSpiritual Healing for the Soul.This book has been the basis for many years of intense spiritual practice and spiritual study\n",
      "\n",
      "üî∏ Pergunta: What is special about the Mexico The Beautiful Cookbook?\n",
      "üîπ Resposta depois do fine-tuning: A Cookery History of the Mexican Country\n",
      "The New Mexican Cook Book, with the first recipes from the country's most famous chefs\n",
      "With the latest information on the food of Mexico, this is the perfect companion to the book, which is packed with beautiful photographs and recipes. With the best in food, recipes, and techniques, the New Mexico Cookbooks are sure to inspire and entertain the hungry. From traditional Mexican dishes such as a pocho or las cinco to traditional dishes like pinto beans and pomegranate bread, Mexican cuisine is still the same. The New Mexicans cookbook is an exceptional addition to any library. A\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Carrega o modelo fine-tuned\n",
    "tokenizer_finetuned, model_finetuned = load_model('fine_tuned_model')\n",
    "\n",
    "print(\"üß† üöÄ Respostas DEPOIS do fine-tuning:\\n\")\n",
    "\n",
    "for pergunta in perguntas:\n",
    "    resposta = testar_modelo(pergunta, tokenizer_finetuned, model_finetuned)\n",
    "    print(f\"üî∏ Pergunta: {pergunta}\")\n",
    "    print(f\"üîπ Resposta depois do fine-tuning: {resposta}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ea36de6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo carregado. Voc√™ pode come√ßar a fazer perguntas. Digite 'sair' para encerrar.\n",
      "Resposta: You are an assistant that answers questions based on the product description.\n",
      "\n",
      "    Question: The Best of India: A Cookbook\n",
      "    Answer: A cookbook to help you cook and drink as well as enjoy the pleasures of India. --Greece Times, January 2009In this cookbook, Indian food expert Darshan Pradeep offers an array of recipes to help you discover and enjoy your own India, including the best of India. --Indian Express, January 2009In this cookbook, Indian food expert Darshan Pradeep offers an array of recipes to help you discover and enjoy your own India, including the best of India. --Indian Express, January 2009In this cookbook, Indian food expert D\n",
      "Encerrando o gerador de respostas.\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# üß† Intera√ß√£o com o Modelo Treinado\n",
    "# =========================================\n",
    "\n",
    "model_path = 'fine_tuned_model'\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    raise FileNotFoundError(\n",
    "        f\"A pasta {model_path} n√£o foi encontrada. Execute o fine-tuning primeiro.\"\n",
    "    )\n",
    "\n",
    "tokenizer, model = load_model(model_path)\n",
    "\n",
    "print(\"Modelo carregado. Voc√™ pode come√ßar a fazer perguntas. Digite 'sair' para encerrar.\")\n",
    "\n",
    "while True:\n",
    "    pergunta = input(\"Fa√ßa sua pergunta: \")\n",
    "    if pergunta.lower() == 'sair':\n",
    "        print(\"Encerrando o gerador de respostas.\")\n",
    "        break\n",
    "    resposta = gerar_resposta(pergunta, tokenizer, model)\n",
    "    print(f\"Resposta: {resposta}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef06ed1",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### üîπ **Conclus√£o**\n",
    "```markdown\n",
    "## Conclus√£o\n",
    "\n",
    "Este projeto demonstrou, na pr√°tica, o processo de fine-tuning de um modelo de linguagem baseado no dataset AmazonTitles-1.3MM. Mesmo utilizando um modelo leve como o DistilGPT-2, foi poss√≠vel observar melhorias claras na gera√ß√£o de respostas ap√≥s o treinamento.\n",
    "\n",
    "Para aplica√ß√µes em produ√ß√£o ou cen√°rios mais exigentes, recomenda-se utilizar foundation models mais robustos, como os da fam√≠lia **Llama** ou **Mistral**.\n",
    "\n",
    "O c√≥digo completo e a demonstra√ß√£o podem ser acessados nos links fornecidos no arquivo de entrega ou no github: https://github.com/UnB-EngEnerg-180028863/Terceiro-Tech-Challenge."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
