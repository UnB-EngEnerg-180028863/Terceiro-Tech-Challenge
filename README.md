# ğŸš€ Tech Challenge - Fine-Tuning de Foundation Model

**Aluno:** VinÃ­cius Oliveira Litran Andrade
**InstituiÃ§Ã£o:** FIAP - PÃ³s-graduaÃ§Ã£o em InteligÃªncia Artificial (IA para Devs)

---

## ğŸ“œ DescriÃ§Ã£o do Projeto

Este projeto consiste na aplicaÃ§Ã£o de **fine-tuning de um modelo de linguagem** utilizando o dataset **AmazonTitles-1.3MM**, que contÃ©m tÃ­tulos e descriÃ§Ãµes de produtos da Amazon.

O objetivo Ã© treinar o modelo para que, ao receber uma pergunta baseada no tÃ­tulo de um produto, ele consiga gerar uma resposta coerente com base na descriÃ§Ã£o aprendida durante o treinamento.

---

## ğŸ”¥ Justificativa do Modelo

O desafio sugere o uso de foundation models como **Llama**, **Mistral** ou **BERT**. Contudo, devido a limitaÃ§Ãµes de hardware local (como RTX 4070, RTX 3060 ou RTX 2060), o modelo utilizado foi o **DistilGPT-2**, uma versÃ£o leve e otimizada do GPT-2, com aproximadamente **82 milhÃµes de parÃ¢metros** (em comparaÃ§Ã£o com os 124 milhÃµes do GPT-2 original).

Essa escolha permite realizar o fine-tuning e a geraÃ§Ã£o de respostas de forma eficiente, sendo leve e rÃ¡pido de treinar, menos demandante em termos de recursos computacionais e viÃ¡vel para execuÃ§Ã£o local, sem custos adicionais com infraestrutura em nuvem. Embora o desempenho nÃ£o seja comparÃ¡vel a modelos maiores, ele Ã© plenamente suficiente para validar o pipeline completo de fine-tuning aplicado ao dataset _AmazonTitles-1.3MM_.

---

## ğŸ—‚ï¸ Estrutura do Projeto

â”œâ”€â”€ LF-Amazon-1.3M/ # Dataset (trn.json) ğŸ”— [LF-Amazon-1.3M](https://drive.google.com/file/d/12zH4mL2RX8iSvH0VCNnd3QxO4DzuHWnK/view)

â”œâ”€â”€ fine_tune_data.jsonl # Dados preparados para o treino 

â”œâ”€â”€ fine_tuned_model/ # Modelo treinado

â”œâ”€â”€ tech_challenge.ipynb # Notebook com todo o cÃ³digo 

â”œâ”€â”€ README.md # DocumentaÃ§Ã£o do projeto

---

## âš™ï¸ Pipeline do Projeto

### ğŸ”¹ 1. PrÃ©-processamento dos Dados
- Leitura do arquivo `trn.json` 
- Limpeza e normalizaÃ§Ã£o dos textos, removendo HTML, caracteres especiais e espaÃ§os desnecessÃ¡rios
- CriaÃ§Ã£o dos prompts no formato:
You are an assistant that answers questions based on the product description.
Question: {title}
Answer: {content}

- LimitaÃ§Ã£o dos textos para um mÃ¡ximo de **256 tokens**

### ğŸ”¹ 2. Fine-Tuning (Treinamento)
- Modelo utilizado: **DistilGPT-2** 
- ParÃ¢metros principais:
  - Ã‰pocas (`num_train_epochs`): `3` 
  - Batch Size (`per_device_train_batch_size`): `16` 
  - Learning Rate (`learning_rate`): `5e-5` 
  - MÃ¡ximo de tokens (`max_length`): `256`
  - `fp16`: `True` (se GPU com CUDA disponÃ­vel)
  - Otimizador (`optim`): `adamw_torch_fused`
  - `save_steps`: `500`
  - `save_total_limit`: `1`
  - `logging_steps`: `100`
  - `weight_decay`: `0.01`
- Processo:
  - Treinamento com os prompts gerados.
  - AvaliaÃ§Ã£o do modelo antes e depois do fine-tuning para analisar a diferenÃ§a do resultado gerado.

### ğŸ”¹ 3. GeraÃ§Ã£o de Respostas
- O usuÃ¡rio fornece uma pergunta baseada no **tÃ­tulo de um produto**.
- O modelo retorna uma **resposta coerente com base na descriÃ§Ã£o aprendida** durante o fine-tuning.

---

## ğŸ’» Tecnologias Utilizadas

- ğŸ Python
- ğŸ¤— Hugging Face Transformers
- ğŸ¤— Hugging Face Datasets
- ğŸ¼ Pandas
- ğŸ”¥ PyTorch
- `tqdm` (para barras de progresso)
- `pathlib` (para manipulaÃ§Ã£o de caminhos)
- `json` (para manipulaÃ§Ã£o de JSON)
- `os` (para interaÃ§Ãµes com o sistema operacional)
- `re` (para expressÃµes regulares)
- `unicodedata` (para normalizaÃ§Ã£o Unicode)
- `html` (para unescape de HTML)

---

## â–¶ï¸ DemonstraÃ§Ã£o em VÃ­deo

ğŸ¥ Link para o vÃ­deo no YouTube:
ğŸ‘‰ [Assista aqui](https://www.youtube.com/SEU_VIDEO_AQUI) *(substituir pelo link real)* 
---

## ğŸ“‚ Link do RepositÃ³rio

ğŸ”— [Acesse o projeto no GitHub](https://github.com/UnB-EngEnerg-180028863/Terceiro-Tech-Challenge) 
